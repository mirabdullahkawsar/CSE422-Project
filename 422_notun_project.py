# -*- coding: utf-8 -*-
"""422 notun project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bYvc1OcugxyzOzf9dbbwshnOR5ktI5pI
"""

# Load the dataset with specified encoding
file_path = '/content/Liver Patient Dataset (LPD)_train.csv'



# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.tree import export_graphviz
import graphviz
from math import sqrt

# Load the dataset
data = pd.read_csv('/content/Liver Patient Dataset (LPD)_train.csv', encoding='latin1')
data2 = pd.read_csv('/content/Liver Patient Dataset (LPD)_train.csv', encoding='latin1')  # For Comparing
data.head(10)

print(data.shape)

print(data.isnull().sum())  #sum null val

data.isnull() #checking null values

# Calculate the percentage of missing values for each column
null_percentages = data.isnull().mean() * 100

# Identify columns with more than 50% null values
columns_to_drop_due_to_nulls = null_percentages[null_percentages > 50].index.tolist()

# Dropping columns with more than 50% null values and irrelevant columns
data.drop(columns=columns_to_drop_due_to_nulls, axis=1, inplace=True)

columns_to_drop_due_to_nulls, data.shape

print(data.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns

for col in data.columns:
    plt.figure()
    sns.histplot(data=data, x=col, hue="Result", kde=True)
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Plotting histograms for the dataset
# %matplotlib inline
data.hist(bins=10, figsize=(20, 15))
plt.show()

print(data.columns)

data = pd.read_csv('/content/Liver Patient Dataset (LPD)_train.csv', encoding='latin1')# Check if 'Gender of the patient' column exists before attempting to plot
if "Gender of the patient" in data.columns:
    data["Gender of the patient"].value_counts().plot(kind="pie", autopct="%.2f%%")
else:
    print("Column 'Gender_of_the_patient' not found in the DataFrame.")

data.columns = [col.strip().replace(" ", "_") for col in data.columns]

# Verify column names
print(data.columns)

# Replacing null values of quantitative features with their mean
quantitative_features = [
    'Age_of_the_patient', 'Total_Bilirubin', 'Direct_Bilirubin',
    'Sgot_Aspartate_Aminotransferase', 'Total_Protiens',
    'A/G_Ratio_Albumin_and_Globulin_Ratio'
]

for feature in quantitative_features:
    if feature in data.columns:
        data[feature] = data[feature].fillna(data[feature].mean())
    else:
        print(f"Feature '{feature}' not found in the dataset!")

# Calculate the sum of missing values for each column
missing_values = data.isnull().sum()

# Display the missing values for each column
print("Missing values per column:")
print(missing_values)

# Identify categorical columns
categorical_columns = data.select_dtypes(include=['object']).columns

# Replace missing values in each categorical column with its mode
for col in categorical_columns:
    data[col] = data[col].fillna(data[col].mode()[0])

# Display the dataset after replacing nulls in categorical columns
print(data.isnull().sum())

# Fill missing values in numerical columns with their mean
data['Alkphos_Alkaline_Phosphotase'] = data['Alkphos_Alkaline_Phosphotase'].fillna(data['Alkphos_Alkaline_Phosphotase'].mean())
data['Sgpt_Alamine_Aminotransferase'] = data['Sgpt_Alamine_Aminotransferase'].fillna(data['Sgpt_Alamine_Aminotransferase'].mean())
data['ALB_Albumin'] = data['ALB_Albumin'].fillna(data['ALB_Albumin'].mean())

# Verify if there are any remaining missing values
print(data.isnull().sum())

# List of quantitative columns
quantitative_columns = [
    'Age_of_the_patient',
    'Total_Bilirubin',
    'Direct_Bilirubin',
    'Alkphos_Alkaline_Phosphotase',
    'Sgpt_Alamine_Aminotransferase',
    'Sgot_Aspartate_Aminotransferase',
    'Total_Protiens',
    'ALB_Albumin',
    'A/G_Ratio_Albumin_and_Globulin_Ratio'
]

# Checking for negative values in quantitative columns
for column in quantitative_columns:
    if (data[column] < 0).any():
        print(f"{column} column has negative values.\n")
    else:
        print(f"{column} column does not have negative values.\n")

# Display the column names in your dataset
print(data.columns)
#target = "Outcome"  # Replace with the actual target column name

# Define the correct target column
target = "Result"  # Assuming 'Result' is your target column

# Define the columns to plot (excluding the target column)
columns_to_plot = list(range(data.shape[1] - 1))  # All columns except the last one ('Result')

# Create subplots
figure, axs = plt.subplots(len(columns_to_plot) // 2 + len(columns_to_plot) % 2,
                            2, figsize=(20, len(columns_to_plot) * 2), sharey=True, squeeze=True)

# Plot each column against the target
for i, column in enumerate(columns_to_plot):
    axs[i // 2, i % 2].bar(data.iloc[:, column], data[target], color='Blue', alpha=0.5)
    axs[i // 2, i % 2].set_xlabel(data.columns[column])
    axs[i // 2, i % 2].set_ylabel(target)
    axs[i // 2, i % 2].set_title(f'{data.columns[column]} vs {target}')

# Adjust layout
figure.subplots_adjust(wspace=0.3, hspace=0.5)

# Show plots
plt.show()

# Compare the null values in pre-processed and unprocessed datasets
NULL_values_comparison = pd.concat(
    [data2.isnull().sum(), data.isnull().sum()], axis=1
)

# Rename columns for clarity
NULL_values_comparison.columns = ['NULL_Values_Unprocessed', 'NULL_Values_Processed']

# Print the comparison
print(NULL_values_comparison)

# Display data types of all columns
print(data.dtypes)

# Identify categorical features (columns with dtype 'object')
categorical_features = data.select_dtypes(include=['object']).columns

# Print the categorical features
print("Categorical Features:")
print(categorical_features)

# Identify categorical features
categorical_features = data.select_dtypes(include=['object']).columns

# Print unique values for each categorical feature
for feature in categorical_features:
    print(f"{feature} categorical features Value:")
    print(data[feature].unique())
    print("----------------------------------------------------------------------------")

for feature in categorical_features:
    print(f"{feature}: {data[feature].nunique()} unique values")

# Specify the categorical column for one-hot encoding
categorical_cols = ['Gender_of_the_patient']

# Perform one-hot encoding on the specified categorical columns
data_encoded = pd.get_dummies(data, columns=categorical_cols, dtype=int)

# Display the first few rows of the encoded dataset
print(data_encoded.head())

# Display information about the dataset after one-hot encoding
data_encoded.info()

from sklearn.preprocessing import StandardScaler
import numpy as np

# Initialize the StandardScaler
scaler = StandardScaler()

# Define batch size
batch_size = 500

# Apply partial fitting to the scaler in batches
for i in range(0, len(data_encoded), batch_size):
    scaler.partial_fit(data_encoded.iloc[i:i+batch_size])

# Transform the dataset using the fitted scaler
X_scaled = scaler.transform(data_encoded)

# Display the transformed data (standardized)
print("Standardized Data:")
print(X_scaled)

# Verify the shape to ensure successful scaling
print(f"Shape of X_scaled: {X_scaled.shape}")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Convert the standardized data back into a DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=data_encoded.columns)

# Extract numeric columns from the original (unprocessed) dataset
numeric_columns = data2.select_dtypes(include=['int64', 'float64']).columns
housing_numeric = data2[numeric_columns]

# Calculate the correlation matrix before preprocessing
corr_before = housing_numeric.corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_before, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix (Before Standardization, Encoding, and Imputation)')
plt.show()

# Convert the standardized array into a DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=data_encoded.columns)

# Calculate the correlation matrix after preprocessing
corr_after = X_scaled_df.corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_after, cmap='GnBu', annot=True, fmt=".2f")
plt.title('Correlation Matrix (After Standardization, Encoding, and Imputation)')
plt.show()

print(data_encoded.columns)

# Check the column names in the data_encoded DataFrame to locate the target variable
data_encoded_columns = data_encoded.columns
data_encoded_columns

from sklearn.model_selection import train_test_split

# Splitting dataset into train (70%) and test (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1337)

# Calculate percentages
train_percentage = len(X_train) / len(X) * 100
test_percentage = len(X_test) / len(X) * 100

# Display results
print(f"Training Set: {len(X_train)} rows ({train_percentage:.2f}%)")
print(f"Test Set: {len(X_test)} rows ({test_percentage:.2f}%)")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

def get_train_test_split(data, target_column, test_size=0.2, random_state=1337):
    """
    Splits the data into training and test sets after one-hot encoding and scaling

    Args:
      data (pd.DataFrame): Input dataframe
      target_column (str): name of the target column
      test_size (float): percentage of data to use for test
      random_state(int): random seed for reproducibility

    Returns:
       X_train, X_test, y_train, y_test (tuple): training and testing splits
    """

    # Perform one-hot encoding
    categorical_cols = ['Gender_of_the_patient']
    data_encoded = pd.get_dummies(data, columns=categorical_cols, dtype=int)


    # Scale the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data_encoded.drop(columns=[target_column]))

    # Prepare X and y
    X = X_scaled
    y = data_encoded[target_column]

    # Perform the train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
    return X_train, X_test, y_train, y_test

# Load the dataset
data = pd.read_csv('/content/Liver Patient Dataset (LPD)_train.csv', encoding='latin1')
data.columns = [col.strip().replace(" ", "_") for col in data.columns]

# Replacing null values of quantitative features with their mean
quantitative_features = [
    'Age_of_the_patient', 'Total_Bilirubin', 'Direct_Bilirubin',
    'Sgot_Aspartate_Aminotransferase', 'Total_Protiens',
    'A/G_Ratio_Albumin_and_Globulin_Ratio'
]

for feature in quantitative_features:
        data[feature] = data[feature].fillna(data[feature].mean())

# Identify categorical columns
categorical_columns = data.select_dtypes(include=['object']).columns

# Replace missing values in each categorical column with its mode
for col in categorical_columns:
    data[col] = data[col].fillna(data[col].mode()[0])

# Fill missing values in numerical columns with their mean
data['Alkphos_Alkaline_Phosphotase'] = data['Alkphos_Alkaline_Phosphotase'].fillna(data['Alkphos_Alkaline_Phosphotase'].mean())
data['Sgpt_Alamine_Aminotransferase'] = data['Sgpt_Alamine_Aminotransferase'].fillna(data['Sgpt_Alamine_Aminotransferase'].mean())
data['ALB_Albumin'] = data['ALB_Albumin'].fillna(data['ALB_Albumin'].mean())

# Define the target variable
target = "Result"
X_train, X_test, y_train, y_test = get_train_test_split(data, target)

# K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
knn_predictions = knn_model.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_predictions)
print("K-Nearest Neighbors Accuracy:", knn_accuracy)

# Decision Tree Classifier (DT)
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_predictions)
print("Decision Tree Accuracy:", dt_accuracy)

# Random Forest (RF)
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print("Random Forest Accuracy:", rf_accuracy)

# K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
knn_predictions = knn_model.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_predictions)
print("K-Nearest Neighbors Accuracy:", knn_accuracy)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
# Decision Tree Classifier (DT)
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_predictions)
print("Decision Tree Accuracy:", dt_accuracy)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Random Forest (RF)
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print("Random Forest Accuracy:", rf_accuracy)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score

# Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

# Convert predictions to discrete categories (e.g., round to nearest integer if the target is categorical)
lr_predictions_rounded = np.round(lr_predictions)

# Calculate accuracy
accuracy = accuracy_score(y_test, lr_predictions_rounded)

# Evaluate regression metrics
mse = mean_squared_error(y_test, lr_predictions)
r2 = r2_score(y_test, lr_predictions)

print("Linear Regression Accuracy (approximated):", accuracy)
print("Linear Regression Mean Squared Error:", mse)
print("Linear Regression R-squared Score:", r2)

# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# import seaborn as sns
# import matplotlib.pyplot as plt
# import numpy as np

# # Initialize models
# models = {
#     "K-Nearest Neighbors": KNeighborsClassifier(),
#     "Decision Tree": DecisionTreeClassifier(random_state=42),
#     "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
# }

# # Train and evaluate classification models
# for model_name, model in models.items():
#     print(f"\nTraining and Testing: {model_name}")

#     # Train the model
#     model.fit(X_train, y_train)

#     # Make predictions
#     y_pred = model.predict(X_test)

#     # Evaluate the model
#     accuracy = accuracy_score(y_test, y_pred)
#     print(f"Accuracy: {accuracy:.2f}")
#     print("Classification Report:")
#     print(classification_report(y_test, y_pred))

#     # Confusion Matrix
#     cm = confusion_matrix(y_test, y_pred)
#     plt.figure(figsize=(8, 6))
#     sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=model.classes_, yticklabels=model.classes_)
#     plt.title(f"Confusion Matrix for {model_name}")
#     plt.xlabel("Predicted")
#     plt.ylabel("Actual")
#     plt.show()

# # Linear Regression
# print("\nTraining and Testing: Linear Regression")
# lr_model = LinearRegression()
# lr_model.fit(X_train, y_train)

# # Make predictions
# lr_predictions = lr_model.predict(X_test)

# # Round predictions to nearest integer for classification-like evaluation
# lr_predictions_rounded = np.round(lr_predictions)

# # Evaluate the model
# lr_accuracy = accuracy_score(y_test, lr_predictions_rounded)
# print(f"Accuracy: {lr_accuracy:.2f}")
# print("Classification Report:")
# print(classification_report(y_test, lr_predictions_rounded))

# # Confusion Matrix for Linear Regression
# lr_cm = confusion_matrix(y_test, lr_predictions_rounded)
# plt.figure(figsize=(8, 6))
# sns.heatmap(lr_cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
# plt.title("Confusion Matrix for Linear Regression")
# plt.xlabel("Predicted")
# plt.ylabel("Actual")
# plt.show()

# # Linear Regression
# print("\nTraining and Testing: Linear Regression")
# lr_model = LinearRegression()
# lr_model.fit(X_train, y_train)

# # Make predictions
# lr_predictions = lr_model.predict(X_test)

# # Convert predictions to discrete categories by rounding to nearest integer
# # Ensure the predictions are integers matching the unique values of the target variable
# lr_predictions_rounded = np.clip(np.round(lr_predictions), a_min=min(y_test), a_max=max(y_test)).astype(int)

# # Evaluate the model
# lr_accuracy = accuracy_score(y_test, lr_predictions_rounded)
# print(f"Accuracy: {lr_accuracy:.2f}")
# print("Classification Report:")
# print(classification_report(y_test, lr_predictions_rounded))

# # Confusion Matrix for Linear Regression
# lr_cm = confusion_matrix(y_test, lr_predictions_rounded)
# plt.figure(figsize=(8, 6))
# sns.heatmap(lr_cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
# plt.title("Confusion Matrix for Linear Regression")
# plt.xlabel("Predicted")
# plt.ylabel("Actual")
# plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize models
models = {
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
}

# Train and evaluate classification models
for model_name, model in models.items():
    print(f"\nTraining and Testing: {model_name}")

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=model.classes_, yticklabels=model.classes_)
    plt.title(f"Confusion Matrix for {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Linear Regression
print("\nTraining and Testing: Linear Regression")
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Make predictions
lr_predictions = lr_model.predict(X_test)

# Convert predictions to discrete categories by rounding to nearest integer
# Ensure the predictions are integers matching the unique values of the target variable
lr_predictions_rounded = np.clip(np.round(lr_predictions), a_min=min(y_test), a_max=max(y_test)).astype(int)

# Evaluate the model
lr_accuracy = accuracy_score(y_test, lr_predictions_rounded)
print(f"Accuracy: {lr_accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, lr_predictions_rounded))

# Confusion Matrix for Linear Regression
lr_cm = confusion_matrix(y_test, lr_predictions_rounded)
plt.figure(figsize=(8, 6))
sns.heatmap(lr_cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title("Confusion Matrix for Linear Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Logistic Regression Model
print("\nTraining and Testing: Logistic Regression")

# Initialize and train the model
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train, y_train)

# Make predictions
y_pred_logistic = logistic_model.predict(X_test)

# Evaluate the model
logistic_accuracy = accuracy_score(y_test, y_pred_logistic)
print(f"Accuracy: {logistic_accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_logistic))

# Confusion Matrix
logistic_cm = confusion_matrix(y_test, y_pred_logistic)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(logistic_cm, annot=True, fmt="d", cmap="Blues", xticklabels=logistic_model.classes_, yticklabels=logistic_model.classes_)
plt.title("Confusion Matrix for Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import numpy as np


def get_train_test_split(data, target_column, test_size=0.2, random_state=1337):
    """
    Splits the data into training and test sets after one-hot encoding and scaling

    Args:
      data (pd.DataFrame): Input dataframe
      target_column (str): name of the target column
      test_size (float): percentage of data to use for test
      random_state(int): random seed for reproducibility

    Returns:
       X_train, X_test, y_train, y_test (tuple): training and testing splits
    """

    # Perform one-hot encoding
    categorical_cols = ['Gender_of_the_patient']
    data_encoded = pd.get_dummies(data, columns=categorical_cols, dtype=int)


    # Scale the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data_encoded.drop(columns=[target_column]))

    # Prepare X and y
    X = X_scaled
    y = data_encoded[target_column]

    # Perform the train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
    return X_train, X_test, y_train, y_test

# Load the dataset
data = pd.read_csv('/content/Liver Patient Dataset (LPD)_train.csv', encoding='latin1')
data.columns = [col.strip().replace(" ", "_") for col in data.columns]

# Replacing null values of quantitative features with their mean
quantitative_features = [
    'Age_of_the_patient', 'Total_Bilirubin', 'Direct_Bilirubin',
    'Sgot_Aspartate_Aminotransferase', 'Total_Protiens',
    'A/G_Ratio_Albumin_and_Globulin_Ratio'
]

for feature in quantitative_features:
        data[feature] = data[feature].fillna(data[feature].mean())

# Identify categorical columns
categorical_columns = data.select_dtypes(include=['object']).columns

# Replace missing values in each categorical column with its mode
for col in categorical_columns:
    data[col] = data[col].fillna(data[col].mode()[0])

# Fill missing values in numerical columns with their mean
data['Alkphos_Alkaline_Phosphotase'] = data['Alkphos_Alkaline_Phosphotase'].fillna(data['Alkphos_Alkaline_Phosphotase'].mean())
data['Sgpt_Alamine_Aminotransferase'] = data['Sgpt_Alamine_Aminotransferase'].fillna(data['Sgpt_Alamine_Aminotransferase'].mean())
data['ALB_Albumin'] = data['ALB_Albumin'].fillna(data['ALB_Albumin'].mean())


# Define the target variable
target = "Result"
X_train, X_test, y_train, y_test = get_train_test_split(data, target)


# Logistic Regression Model
print("\nTraining and Testing: Logistic Regression")

# Initialize and train the model
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train, y_train)

# Make predictions
y_pred_logistic = logistic_model.predict(X_test)

# Evaluate the model
logistic_accuracy = accuracy_score(y_test, y_pred_logistic)
print(f"Accuracy: {logistic_accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_logistic))

# Confusion Matrix
logistic_cm = confusion_matrix(y_test, y_pred_logistic)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(logistic_cm, annot=True, fmt="d", cmap="Blues", xticklabels=logistic_model.classes_, yticklabels=logistic_model.classes_)
plt.title("Confusion Matrix for Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Naive Bayes Model
print("\nTraining and Testing: Naive Bayes")

# Initialize and train the model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Make predictions
y_pred_nb = nb_model.predict(X_test)

# Evaluate the model
nb_accuracy = accuracy_score(y_test, y_pred_nb)
print(f"Accuracy: {nb_accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_nb))

# Confusion Matrix
nb_cm = confusion_matrix(y_test, y_pred_nb)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(nb_cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title("Confusion Matrix for Naive Bayes")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()